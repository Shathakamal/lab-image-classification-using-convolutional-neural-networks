{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Lab : Image Classification using Convolutional Neural Networks\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAzwMYYArxJL"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **In case you use a colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrb20KGMtTFq",
        "outputId": "209b4b6e-2285-445f-9915-0b003682b47b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "ZBG8UF7WrtZQ",
        "outputId": "4ebcde86-8458-4782-b8b9-e70e888b5ce8"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "for i in range(10):\n",
        "    class_indices = np.where(y_train.flatten() == i)[0]\n",
        "    random_indices = np.random.choice(class_indices, 10, replace=False)\n",
        "    for j, img_idx in enumerate(random_indices):\n",
        "        axes[i, j].imshow(x_train[img_idx])\n",
        "        axes[i, j].axis('off')\n",
        "    axes[i, 0].set_ylabel(class_names[i], fontsize=12, rotation=90, labelpad=10)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "\n",
        "Use the input as (32,32,3).\n",
        "\n",
        "The filter maps can then be flattened to provide features to the classifier.\n",
        "\n",
        "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "iSN6riPISBMG",
        "outputId": "cbff5a11-ff73-4fcc-e614-85d306dffd31"
      },
      "outputs": [],
      "source": [
        "# Your code here : import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.backend import clear_session\n",
        "\n",
        "clear_session()\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn8UzPBZugVp",
        "outputId": "77a97087-1b77-4c6e-92a7-1aec81fef1d0"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDqIZWXSrtZR"
      },
      "source": [
        "*   Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sU_PGjq_rtZR",
        "outputId": "163fe3cd-8893-442d-a311-9f01e19e7005"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Cross Entropy Loss Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "\n",
        "Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "\n",
        "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cgca5dUNSFNc",
        "outputId": "d44c5c9c-367c-4df8-a948-9c987a5cbaf0"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "clear_session()\n",
        "\n",
        "model_vgg = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_vgg.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "history_vgg = model_vgg.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_vgg.history['loss'], label='Training Loss')\n",
        "plt.plot(history_vgg.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Cross Entropy Loss Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_vgg.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_vgg.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc2qtU0mUvVA",
        "outputId": "a824f7a8-f227-4235-c21a-30dd506721de"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "model_vgg.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_vgg = model_vgg.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WxTcR_kW12vU",
        "outputId": "69ad2772-984e-464a-c793-9e617ad7652b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize the image data to range [0, 1]\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define Shallow Model\n",
        "def shallow_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Define Deep Model\n",
        "def deep_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Compile both models\n",
        "shallow_model = shallow_model()\n",
        "shallow_model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "deep_model = deep_model()\n",
        "deep_model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train both models for 50 epochs with batch size of 512\n",
        "history_shallow = shallow_model.fit(X_train, y_train, epochs=50, batch_size=512, validation_data=(X_test, y_test))\n",
        "history_deep = deep_model.fit(X_train, y_train, epochs=50, batch_size=512, validation_data=(X_test, y_test))\n",
        "\n",
        "# Plotting Loss and Accuracy Curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Training Loss for both models\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_shallow.history['loss'], label='Shallow Model Loss')\n",
        "plt.plot(history_deep.history['loss'], label='Deep Model Loss')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Training Accuracy for both models\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_shallow.history['accuracy'], label='Shallow Model Accuracy')\n",
        "plt.plot(history_deep.history['accuracy'], label='Deep Model Accuracy')\n",
        "plt.title('Training Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "**Comment on the observation**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "*   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "DObaoxhaSMUg",
        "outputId": "9c2e23cf-99e9-42b0-acbf-38ec923140c6"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict on the test set using the shallow model\n",
        "y_pred_shallow = shallow_model.predict(X_test)\n",
        "y_pred_deep = deep_model.predict(X_test)\n",
        "\n",
        "# Convert predictions to the class with the highest probability\n",
        "y_pred_shallow_classes = y_pred_shallow.argmax(axis=1)\n",
        "y_pred_deep_classes = y_pred_deep.argmax(axis=1)\n",
        "y_true_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Compute confusion matrix for both models\n",
        "cm_shallow = confusion_matrix(y_true_classes, y_pred_shallow_classes)\n",
        "cm_deep = confusion_matrix(y_true_classes, y_pred_deep_classes)\n",
        "\n",
        "# Plotting confusion matrix for shallow model\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cm_shallow, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title('Confusion Matrix - Shallow Model')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "\n",
        "# Plotting confusion matrix for deep model\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(cm_deep, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title('Confusion Matrix - Deep Model')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Comment here :**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        "*    Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WX3_uLSN5I",
        "outputId": "61ae4f8d-6940-471d-ef3b-cc737be98b88"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Evaluate the test accuracy for both models\n",
        "test_loss_shallow, test_acc_shallow = shallow_model.evaluate(X_test, y_test)\n",
        "test_loss_deep, test_acc_deep = deep_model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the test accuracies for both models\n",
        "print(f\"Test Accuracy - Shallow Model: {test_acc_shallow * 100:.2f}%\")\n",
        "print(f\"Test Accuracy - Deep Model: {test_acc_deep * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "\n",
        "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "\n",
        "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "oH4lDVBuVA_Q",
        "outputId": "d2ea0e11-43b4-41b2-f9fb-9c4d8f5b2042"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Clear any previous session to avoid clutter\n",
        "K.clear_session()\n",
        "\n",
        "# Define the VGG architecture\n",
        "def vgg_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # First block: Two convolutional layers with 64 filters\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second block: Two convolutional layers with 128 filters\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third block: Two convolutional layers with 256 filters\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add a dense layer with 128 units\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    # Output classification layer (for 10 classes, change as needed)\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "vgg = vgg_model()\n",
        "\n",
        "# Print the model summary to verify the architecture\n",
        "vgg.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4elnDWnjEbmO",
        "outputId": "74081ab6-0f36-47d0-a14b-6cef729bff2d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_data=(x_test, y_test))\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for CIFAR-10')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlzFt0SXGDQ"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in a network? Why is it needed?\n",
        "\n",
        "Activation functions introduce non-linearity into the network, enabling it to learn complex patterns and make non-linear decisions.\n",
        "\n",
        "*   What is the difference between sigmoid activation and softmax activation?\n",
        "\n",
        "Sigmoid is used for binary classification, outputting values between 0 and 1.\n",
        "Softmax is used for multi-class classification, converting logits into probabilities that sum to 1, making it easier to determine the most likely class.\n",
        "*  What is the difference between categorical crossentropy and binary crossentropy loss?\n",
        "\n",
        "Binary crossentropy is used for binary classification (two classes).\n",
        "Categorical crossentropy is used for multi-class classification with one-hot encoded labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Write the answers below :**\n",
        "\n",
        "1 - Use of activation functions:\n",
        "\n",
        "_ Activation functions introduce non-linearity into the network, allowing it to learn complex patterns and make non-linear decisions. Without them, the network would behave like a simple linear model. _\n",
        "\n",
        "2 - Key Differences between sigmoid and softmax:\n",
        "\n",
        "_ Sigmoid is used for binary classification, mapping values between 0 and 1, while Softmax is used for multi-class classification, converting logits into probabilities that sum to 1, helping determine the most probable class. _\n",
        "\n",
        "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
        "\n",
        "_ Binary crossentropy is used for two-class classification problems, while categorical crossentropy is used for multi-class classification with one-hot encoded labels\n",
        "\n",
        "\n",
        "_\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
